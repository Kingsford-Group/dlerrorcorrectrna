# make_prediction_inputs.py
#
# Name: Laura Tung
#
# Usage: python make_prediction_inputs.py <final_clusters_sizes> <fullpath_outdir> <n_cores> <merge_csv_files>
#
# <final_clusters_sizes>: the clusters sizes file generated by sort_clusters_reads.py: final_clusters_sizes
# <fullpath_outdir>: the full-path output directory for saving the inputs data.
# <merge_csv_files>: "yes" or "no". If "yes", merge csv files for each cluster; if "no", load the already merged csv for each cluster.

#import pdb; pdb.set_trace() # Uncomment to debug code using pdb (like gdb)

import sys
import numpy as np
import pandas as pd

import pickle
import ast
import math

import subprocess
import multiprocessing
import time

from sklearn.preprocessing import OneHotEncoder



kmer_bases = ['A', 'C', 'G', 'T']
sequence_bases = ['A', 'C', 'G', 'T', '-']
base_code = {
        'A': 0,
        'C': 1,
        'G': 2,
        'T': 3,
        '-': 4
    }
strand_code = {'+': 0, '-': 1}

    

def encode_sequences(onehot_encoder, max_seq_length, seq_bases, base_code):
    
    integer_bases = [base_code[base] for base in seq_bases]
    bases_array = np.array(integer_bases).reshape(len(integer_bases), 1)
    
    onehot_encoded_seq = onehot_encoder.transform(bases_array)
    
    if max_seq_length == onehot_encoded_seq.shape[0]:
        seq_array = onehot_encoded_seq
    else:
        # padding -1
        seq_array = - np.ones((max_seq_length, onehot_encoded_seq.shape[-1]))
        seq_array[:onehot_encoded_seq.shape[0], :] = onehot_encoded_seq
        
    return seq_array


def encode_kmers(onehot_encoder, k, bases, side):
    
    bases_array = np.array(bases).reshape(len(bases), 1)
    
    onehot_encoded_seq = onehot_encoder.transform(bases_array)
        
    if k == onehot_encoded_seq.shape[0]:
        seq_array = onehot_encoded_seq
    else:
        # padding -1
        seq_array = - np.ones((k, onehot_encoded_seq.shape[-1]))
        if side == "left":
            seq_array[k - onehot_encoded_seq.shape[0]:, :] = onehot_encoded_seq
        elif side == "right":
            seq_array[:onehot_encoded_seq.shape[0], :] = onehot_encoded_seq
        else:
            print("Error: invalid argument for side.")
            
    return seq_array


def fit_onehot_encoder_kmers(X_seq):
    
    X = np.array(X_seq).reshape(len(X_seq), 1)
    
    onehot_encoder = OneHotEncoder(categories='auto', sparse=False)
    onehot_encoder.fit(X)
    
    return onehot_encoder


def fit_onehot_encoder_sequences(X_seq, base_code):
    
    integer_seq = [base_code[base] for base in X_seq]
    X = np.array(integer_seq).reshape(len(integer_seq), 1)
    
    onehot_encoder = OneHotEncoder(categories='auto', sparse=False)
    onehot_encoder.fit(X)
    
    return onehot_encoder


def get_kmers_for_read_seq(read_seq, k, kmer_onehot_encoder, kmer_bases):
    
    left_kmers_list = []
    right_kmers_list = []
    
    for i in range(len(read_seq)):
        left_kmer = []
        right_kmer = []
        
        # right kmer
        j = 1
        while (len(right_kmer) < k) and (i+j < len(read_seq)):
            if read_seq[i+j] != '-':
                right_kmer.append(read_seq[i+j])
            j += 1
        
        if right_kmer == []:
            right_kmer_array = - np.ones((k, len(kmer_bases)))
        else:
            right_kmer_array = encode_kmers(kmer_onehot_encoder, k, right_kmer, "right")       
        
        right_kmers_list.append(right_kmer_array)
        
        # left kmer
        j = 1
        while (len(left_kmer) < k) and (i-j >= 0):
            if read_seq[i-j] != '-':
                left_kmer.append(read_seq[i-j])
            j += 1
        left_kmer.reverse()

        if left_kmer == []:
            left_kmer_array = - np.ones((k, len(kmer_bases)))
        else:
            left_kmer_array = encode_kmers(kmer_onehot_encoder, k, left_kmer, "left")
        
        left_kmers_list.append(left_kmer_array)
            
    return left_kmers_list, right_kmers_list           


def normalize_data_columns(merged_df, max_spoa_size):
    
    # number_spoa_reads
    merged_df["number_spoa_reads"] = merged_df["number_spoa_reads"]/max_spoa_size
    
    # for read_length, we normalize it using max_read_length at the prediction time: 
    # normalize the first column of rest_features_input.npy array before model.predict().
    #merged_df["read_length"] = merged_df["read_length"]/max_read_length
    
    # for read_quality, consensus_avg_quality:
    # divide them by max_quality when putting data into np.array (ast.literal_eval into list) in construct_inputs_arrays() function
    
    return merged_df


def construct_inputs_arrays(samples_df, kmer_onehot_encoder, sequence_onehot_encoder, k, max_seq_length, max_quality, kmer_bases, base_code, strand_code):
    
    left_kmers_input = []
    right_kmers_input = []
    read_features_input = []
    consensus_features_input = []
    rest_features_input = []
    
    read_rows_dict = {}
    row_id = 0
    
    
    for i in range(len(samples_df)):
        read_name = samples_df.loc[i, 'read_name']
        rows_this_read = []
        
        read_seq = ast.literal_eval(samples_df.loc[i, 'read_sequence'])
        consensus_seq = ast.literal_eval(samples_df.loc[i, 'consensus_sequence'])
        read_qual = ast.literal_eval(samples_df.loc[i, 'read_quality'])
        consensus_occur_freq = ast.literal_eval(samples_df.loc[i, 'consensus_occur_frequency'])
        consensus_avg_qual = ast.literal_eval(samples_df.loc[i, 'consensus_avg_quality'])
        
        seq_length = len(read_seq)
        base_position = list(range(seq_length))
    
        # normalize read_qual and consensus_avg_qual by max_quality, normalize base_position by seq_length-1
        read_qual = np.array(read_qual)/max_quality
        consensus_avg_qual = np.array(consensus_avg_qual)/max_quality
        base_position = np.array(base_position)/(seq_length-1)
        
        # get left and right kmers
        left_kmers_list, right_kmers_list = get_kmers_for_read_seq(read_seq, k, kmer_onehot_encoder, kmer_bases)
        
        # features for the whole read: make them to go with timesteps
        read_length = [samples_df.loc[i, 'read_length']]*seq_length
        GC_content = [samples_df.loc[i, 'GC_content']]*seq_length
        num_spoa_reads = [samples_df.loc[i, 'number_spoa_reads']]*seq_length
        strand = [float(strand_code[samples_df.loc[i, 'strand']])]*seq_length
                
        # divide longer reads into segments of max_seq_length
        if seq_length > max_seq_length:
            num_segments = math.ceil(seq_length/max_seq_length)
            for n in range(num_segments-1):   
                start = n*max_seq_length
                end = (n+1)*max_seq_length
                
                segm_read_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, read_seq[start:end], base_code)
                segm_consensus_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, consensus_seq[start:end], base_code)
                
                segm_read_qual = make_array_and_pad(read_qual[start:end], max_seq_length)
                segm_consensus_avg_qual = make_array_and_pad(consensus_avg_qual[start:end], max_seq_length)
                segm_base_position = make_array_and_pad(base_position[start:end], max_seq_length)
                segm_consensus_occur_freq = make_array_and_pad(np.array(consensus_occur_freq[start:end]), max_seq_length) 
                
                segm_read_length = make_array_and_pad(np.array(read_length[start:end]), max_seq_length)
                segm_GC_content = make_array_and_pad(np.array(GC_content[start:end]), max_seq_length)
                segm_num_spoa_reads = make_array_and_pad(np.array(num_spoa_reads[start:end]), max_seq_length)
                segm_strand = make_array_and_pad(np.array(strand[start:end]), max_seq_length)
                
                segm_left_kmers = pad_3d_array(np.array(left_kmers_list[start:end]), max_seq_length)
                segm_right_kmers = pad_3d_array(np.array(right_kmers_list[start:end]), max_seq_length)
                                
                left_kmers_input.append(segm_left_kmers)
                right_kmers_input.append(segm_right_kmers)
                
                joint_read_features = np.concatenate((segm_read_seq, segm_read_qual, segm_base_position), axis=1)
                read_features_input.append(joint_read_features)
                
                joint_consensus_features = np.concatenate((segm_consensus_seq, segm_consensus_occur_freq, segm_consensus_avg_qual), axis=1)
                consensus_features_input.append(joint_consensus_features)
                
                joint_rest_features = np.concatenate((segm_read_length, segm_GC_content, segm_num_spoa_reads, segm_strand), axis=1)
                rest_features_input.append(joint_rest_features)
                                
                rows_this_read.append(row_id)
                row_id += 1
            
            # last segment
            final_start = (num_segments-1)*max_seq_length

            segm_read_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, read_seq[final_start:], base_code)
            segm_consensus_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, consensus_seq[final_start:], base_code)
            
            segm_read_qual = make_array_and_pad(read_qual[final_start:], max_seq_length)
            segm_consensus_avg_qual = make_array_and_pad(consensus_avg_qual[final_start:], max_seq_length)
            segm_base_position = make_array_and_pad(base_position[final_start:], max_seq_length)
            segm_consensus_occur_freq = make_array_and_pad(np.array(consensus_occur_freq[final_start:]), max_seq_length) 
            
            segm_read_length = make_array_and_pad(np.array(read_length[final_start:]), max_seq_length)
            segm_GC_content = make_array_and_pad(np.array(GC_content[final_start:]), max_seq_length)
            segm_num_spoa_reads = make_array_and_pad(np.array(num_spoa_reads[final_start:]), max_seq_length)
            segm_strand = make_array_and_pad(np.array(strand[final_start:]), max_seq_length)
            
            segm_left_kmers = pad_3d_array(np.array(left_kmers_list[final_start:]), max_seq_length)
            segm_right_kmers = pad_3d_array(np.array(right_kmers_list[final_start:]), max_seq_length)
                        
            left_kmers_input.append(segm_left_kmers)
            right_kmers_input.append(segm_right_kmers)
            
            joint_read_features = np.concatenate((segm_read_seq, segm_read_qual, segm_base_position), axis=1)
            read_features_input.append(joint_read_features)
            
            joint_consensus_features = np.concatenate((segm_consensus_seq, segm_consensus_occur_freq, segm_consensus_avg_qual), axis=1)
            consensus_features_input.append(joint_consensus_features)
            
            joint_rest_features = np.concatenate((segm_read_length, segm_GC_content, segm_num_spoa_reads, segm_strand), axis=1)
            rest_features_input.append(joint_rest_features)
                        
            rows_this_read.append(row_id)
            row_id += 1
            
        else:
            # take the whole sequence
            segm_read_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, read_seq, base_code)
            segm_consensus_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, consensus_seq, base_code)
            
            segm_read_qual = make_array_and_pad(read_qual, max_seq_length)
            segm_consensus_avg_qual = make_array_and_pad(consensus_avg_qual, max_seq_length)
            segm_base_position = make_array_and_pad(base_position, max_seq_length)
            segm_consensus_occur_freq = make_array_and_pad(np.array(consensus_occur_freq), max_seq_length) 
            
            segm_read_length = make_array_and_pad(np.array(read_length), max_seq_length)
            segm_GC_content = make_array_and_pad(np.array(GC_content), max_seq_length)
            segm_num_spoa_reads = make_array_and_pad(np.array(num_spoa_reads), max_seq_length)
            segm_strand = make_array_and_pad(np.array(strand), max_seq_length)
            
            segm_left_kmers = pad_3d_array(np.array(left_kmers_list), max_seq_length)
            segm_right_kmers = pad_3d_array(np.array(right_kmers_list), max_seq_length)
                        
            left_kmers_input.append(segm_left_kmers)
            right_kmers_input.append(segm_right_kmers)
            
            joint_read_features = np.concatenate((segm_read_seq, segm_read_qual, segm_base_position), axis=1)
            read_features_input.append(joint_read_features)
            
            joint_consensus_features = np.concatenate((segm_consensus_seq, segm_consensus_occur_freq, segm_consensus_avg_qual), axis=1)
            consensus_features_input.append(joint_consensus_features)
            
            joint_rest_features = np.concatenate((segm_read_length, segm_GC_content, segm_num_spoa_reads, segm_strand), axis=1)
            rest_features_input.append(joint_rest_features)
                        
            rows_this_read.append(row_id)
            row_id += 1
            
        read_rows_dict[read_name] = rows_this_read
        
    return np.array(left_kmers_input), np.array(right_kmers_input), np.array(read_features_input), np.array(consensus_features_input), np.array(rest_features_input), read_rows_dict
    

def make_array_and_pad(feat_array, max_seq_length):
    
    feat_array = feat_array.reshape(len(feat_array), 1)
    
    if max_seq_length == feat_array.shape[0]:
        seq_array = feat_array
    else:
        # padding -1
        seq_array = - np.ones((max_seq_length, feat_array.shape[-1]))
        seq_array[:feat_array.shape[0], :] = feat_array
        
    return seq_array    


def pad_3d_array(feat_array, max_seq_length):
    
    if max_seq_length == feat_array.shape[0]:
        seq_array = feat_array
    else:
        # padding -1
        seq_array = - np.ones((max_seq_length, feat_array.shape[1], feat_array.shape[2]))
        seq_array[:feat_array.shape[0], :] = feat_array
        
    return seq_array 


def make_inputs_one_cluster(fail_list, cluster_id, cluster_size, fullpath_outdir, max_spoa_size, max_quality, k, max_seq_length, kmer_bases, sequence_bases, merge_csv_files, base_code, strand_code):
    
    print("Cluster", cluster_id)
    
    try:
        if merge_csv_files == "yes":
            if cluster_size <= max_spoa_size:
                outdir = str(cluster_id) + ".spoa_output"
                features_csv = str(cluster_id) + ".spoa_sma_output_features.csv"
                
                subprocess.call("ln -s " + outdir + "/" + features_csv + " .", shell=True)
                
                merged_df = pd.read_csv(features_csv, sep=',', low_memory=False)
                
            else:
                # top max_spoa_size reads in the cluster
                outdir = str(cluster_id) + ".top.spoa_output"
                features_csv = str(cluster_id) + ".top.spoa_sma_output_features.csv"
                
                merged_df = pd.read_csv(outdir + "/" + features_csv, sep=',', low_memory=False)
                
                # rest of reads after top max_spoa_size reads in the cluster
                rest_dir = str(cluster_id) + ".rest_reads"
                            
                for i in range(max_spoa_size+1, cluster_size+1):
                    print("Read", i)
                                  
                    sub_outdir = rest_dir + "/" + str(i) + ".spoa_output"
                    sub_features_csv = str(i) + ".spoa_sma_output_features.csv"
                    
                    sub_df = pd.read_csv(sub_outdir + "/" + sub_features_csv, sep=',', low_memory=False)
                    
                    if sub_df.shape[0] == 0:
                        continue
                    else:
                        merged_df = merged_df.append(sub_df)
                        
                merged_df = merged_df.reset_index(drop=True)
                
                merged_features_csv = str(cluster_id) + ".spoa_sma_output_features.csv"
                pd.DataFrame.to_csv(merged_df, path_or_buf=merged_features_csv, index=False)
        else:
            features_csv = str(cluster_id) + ".spoa_sma_output_features.csv"
            merged_df = pd.read_csv(features_csv, sep=',', low_memory=False)
            
        # now we have got the data from this cluster
        print("merged_df:")
        print(merged_df.info())
        
        # normalize data columns
        merged_df = normalize_data_columns(merged_df, max_spoa_size)
        
        # one-hot encoders
        kmer_onehot_encoder = fit_onehot_encoder_kmers(kmer_bases)
        sequence_onehot_encoder = fit_onehot_encoder_sequences(sequence_bases, base_code)
        
        prediction_dir = fullpath_outdir + "/" + str(cluster_id) + ".prediction"
        subprocess.call("mkdir -p " + prediction_dir, shell=True)
        
        # construct inputs arrays for this cluster
        left_kmers_input, right_kmers_input, read_features_input, consensus_features_input, rest_features_input, read_rows_dict = \
        construct_inputs_arrays(merged_df, kmer_onehot_encoder, sequence_onehot_encoder, k, max_seq_length, max_quality, kmer_bases, base_code, strand_code)
        print("left_kmers_input shape:", left_kmers_input.shape)
        print("right_kmers_input shape:", right_kmers_input.shape)
        print("read_features_input shape:", read_features_input.shape)
        print("consensus_features_input shape:", consensus_features_input.shape)
        print("rest_features_input shape:", rest_features_input.shape)
        print("read_rows_dict size:", len(read_rows_dict))
        np.save(prediction_dir + "/left_kmers_input", left_kmers_input)
        np.save(prediction_dir + "/right_kmers_input", right_kmers_input)
        np.save(prediction_dir + "/read_features_input", read_features_input)
        np.save(prediction_dir + "/consensus_features_input", consensus_features_input)
        np.save(prediction_dir + "/rest_features_input", rest_features_input)
        pickle_out = open(prediction_dir + "/read_rows_dict.pickle","wb")
        pickle.dump(read_rows_dict, pickle_out)
        pickle_out.close()
        
        return 0
    except:
        fail_list.append(cluster_id)
        return 1
    
    
def make_inputs_for_clusters(clusters_sizes_array, n_cores, fullpath_outdir, max_spoa_size, max_quality, k, max_seq_length, merge_csv_files):
    
    context = multiprocessing.get_context("spawn")
    pool = context.Pool(processes=n_cores)
    manager = multiprocessing.Manager()
    fail_list = manager.list()    
    
    for i in range(clusters_sizes_array.shape[0]):
        cluster_id = clusters_sizes_array[i,0]
        cluster_size = clusters_sizes_array[i,1]
        
        if n_cores <= 1:
            make_inputs_one_cluster(fail_list, cluster_id, cluster_size, fullpath_outdir, max_spoa_size, max_quality, k, max_seq_length, kmer_bases, sequence_bases, merge_csv_files, base_code, strand_code)
        else:
            pool.apply_async(make_inputs_one_cluster, (fail_list, cluster_id, cluster_size, fullpath_outdir, max_spoa_size, max_quality, k, max_seq_length, kmer_bases, sequence_bases, merge_csv_files, base_code, strand_code,))        
    
    pool.close()
    pool.join()

    time.sleep(1)
    if len(fail_list) > 0:
        print('Warning: ' + str(len(fail_list)) + ' clusters inputs could not be made:')
        print(fail_list)
                
    return None
    

if __name__ == "__main__":

    clusters_sizes_file = sys.argv[1]
    fullpath_outdir = sys.argv[2]
    n_cores = int(sys.argv[3])
    merge_csv_files = sys.argv[4]
    
    
    # constants
    max_spoa_size = 800
    max_quality = 93
    
    k = 5
    max_seq_length = 600
    

    # get the clusters sizes info
    clusters_sizes_array = np.loadtxt(clusters_sizes_file, dtype='int')
    print("clusters_sizes_array shape:", clusters_sizes_array.shape)
    
    # make inputs for clusters
    make_inputs_for_clusters(clusters_sizes_array, n_cores, fullpath_outdir, max_spoa_size, max_quality, k, max_seq_length, merge_csv_files)





      

    
    