# make_model_inputs_integer_label_80x.py
#
# Name: Laura Tung
#
# Usage: python make_model_inputs_integer_label_80x.py <final_clusters_sizes> <fullpath_outdir>
#
# <final_clusters_sizes>: the clusters sizes file generated by sort_clusters_reads.py: final_clusters_sizes
# <fullpath_outdir>: the full-path output directory for saving the inputs data.

#import pdb; pdb.set_trace() # Uncomment to debug code using pdb (like gdb)

import sys
import numpy as np
import pandas as pd

import pickle
import ast
import math
import random

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.utils.class_weight import compute_class_weight

#from tensorflow.keras.utils import to_categorical


kmer_bases = ['A', 'C', 'G', 'T']
sequence_bases = ['A', 'C', 'G', 'T', '-']
base_code = {
        'A': 0,
        'C': 1,
        'G': 2,
        'T': 3,
        '-': 4
    }
strand_code = {'+': 0, '-': 1}
label_code = {
        'Insertion': 0,
        'Substituted_A': 1,
        'Substituted_C': 2,
        'Substituted_G': 3,
        'Substituted_T': 4,
        'Deleted_A': 5,
        'Deleted_C': 6,
        'Deleted_G': 7,
        'Deleted_T': 8,
        'No_correction': 9
    }


def encode_labels(labels_list, max_seq_length):
    
    # encoded_labels = to_categorical(labels_list, num_classes=len(label_code))
    encoded_labels = np.array(labels_list)

    if max_seq_length == encoded_labels.shape[0]:
        label_array = encoded_labels
    else:
        # padding -1
        #label_array = - np.ones((max_seq_length, encoded_labels.shape[-1]))
        #label_array[:encoded_labels.shape[0], :] = encoded_labels
        label_array = np.zeros(max_seq_length, dtype='int')
        label_array[:encoded_labels.shape[0]] = encoded_labels  
        
    return label_array    
    

def encode_sequences(onehot_encoder, max_seq_length, seq_bases):
    
    integer_bases = [base_code[base] for base in seq_bases]
    bases_array = np.array(integer_bases).reshape(len(integer_bases), 1)
    
    onehot_encoded_seq = onehot_encoder.transform(bases_array)
    
    if max_seq_length == onehot_encoded_seq.shape[0]:
        seq_array = onehot_encoded_seq
    else:
        # padding -1
        seq_array = - np.ones((max_seq_length, onehot_encoded_seq.shape[-1]))
        seq_array[:onehot_encoded_seq.shape[0], :] = onehot_encoded_seq
        
    return seq_array


def encode_kmers(onehot_encoder, k, bases, side):
    
    bases_array = np.array(bases).reshape(len(bases), 1)
    
    onehot_encoded_seq = onehot_encoder.transform(bases_array)
        
    if k == onehot_encoded_seq.shape[0]:
        seq_array = onehot_encoded_seq
    else:
        # padding -1
        seq_array = - np.ones((k, onehot_encoded_seq.shape[-1]))
        if side == "left":
            seq_array[k - onehot_encoded_seq.shape[0]:, :] = onehot_encoded_seq
        elif side == "right":
            seq_array[:onehot_encoded_seq.shape[0], :] = onehot_encoded_seq
        else:
            print("Error: invalid argument for side.")
            
    return seq_array


def fit_onehot_encoder_kmers(X_seq):
    
    X = np.array(X_seq).reshape(len(X_seq), 1)
    
    onehot_encoder = OneHotEncoder(categories='auto', sparse=False)
    onehot_encoder.fit(X)
    
    return onehot_encoder


def fit_onehot_encoder_sequences(X_seq):
    
    integer_seq = [base_code[base] for base in X_seq]
    X = np.array(integer_seq).reshape(len(integer_seq), 1)
    
    onehot_encoder = OneHotEncoder(categories='auto', sparse=False)
    onehot_encoder.fit(X)
    
    return onehot_encoder


def get_kmers_for_read_seq(read_seq, k, kmer_onehot_encoder):
    
    left_kmers_list = []
    right_kmers_list = []
    
    for i in range(len(read_seq)):
        left_kmer = []
        right_kmer = []
        
        # right kmer
        j = 1
        while (len(right_kmer) < k) and (i+j < len(read_seq)):
            if read_seq[i+j] != '-':
                right_kmer.append(read_seq[i+j])
            j += 1
        
        if right_kmer == []:
            right_kmer_array = - np.ones((k, len(kmer_bases)))
        else:
            right_kmer_array = encode_kmers(kmer_onehot_encoder, k, right_kmer, "right")       
        
        right_kmers_list.append(right_kmer_array)
        
        # left kmer
        j = 1
        while (len(left_kmer) < k) and (i-j >= 0):
            if read_seq[i-j] != '-':
                left_kmer.append(read_seq[i-j])
            j += 1
        left_kmer.reverse()

        if left_kmer == []:
            left_kmer_array = - np.ones((k, len(kmer_bases)))
        else:
            left_kmer_array = encode_kmers(kmer_onehot_encoder, k, left_kmer, "left")
        
        left_kmers_list.append(left_kmer_array)
            
    return left_kmers_list, right_kmers_list           


def normalize_data_columns(merged_df, max_spoa_size):
    
    # number_spoa_reads
    merged_df["number_spoa_reads"] = merged_df["number_spoa_reads"]/max_spoa_size
    
    # read_length
    max_read_length = max(merged_df["read_length"])
    min_read_length = min(merged_df["read_length"])
    merged_df["read_length"] = merged_df["read_length"]/max_read_length
    
    # for read_quality, consensus_avg_quality:
    # divide them by max_quality when putting data into np.array (ast.literal_eval into list) in construct_inputs_arrays() function
    
    return merged_df, max_read_length, min_read_length


def get_data_from_selected_clusters(selected_clusters_sizes_array, max_spoa_size, num_selected_reads_per_cluster):
    
    merged_df = pd.DataFrame()
    
    for i in range(selected_clusters_sizes_array.shape[0]):
        cluster_id = selected_clusters_sizes_array[i,0]
        cluster_size = selected_clusters_sizes_array[i,1]

        if cluster_size <= max_spoa_size:
            outdir = str(cluster_id) + ".spoa_output"
            features_file = outdir + "/" + str(cluster_id) + ".spoa_sma_output_features_labels.csv"
        else:
            outdir = str(cluster_id) + ".top.spoa_output"
            features_file = outdir + "/" + str(cluster_id) + ".top.spoa_sma_output_features_labels.csv"
            
        features_labels_df = pd.read_csv(features_file, sep=',', low_memory=False) 
        
        if features_labels_df.shape[0] == 0:
            continue
        elif num_selected_reads_per_cluster < features_labels_df.shape[0]:
            # randomly sample reads from this cluster
            sampled_features_labels_df = features_labels_df.sample(n=num_selected_reads_per_cluster, random_state=1)
            merged_df = merged_df.append(sampled_features_labels_df)
        else:
            merged_df = merged_df.append(features_labels_df)
            
    merged_df = merged_df.reset_index(drop=True)
    
    return merged_df


def construct_inputs_arrays(samples_df, kmer_onehot_encoder, sequence_onehot_encoder, k, max_seq_length, max_quality):
    
    left_kmers_input = []
    right_kmers_input = []
    read_features_input = []
    consensus_features_input = []
    rest_features_input = []
    true_labels_out = []
    
    read_rows_dict = {}
    row_id = 0
    
    y = []
    
    for i in range(len(samples_df)):
        read_name = samples_df.loc[i, 'read_name']
        rows_this_read = []
        
        read_seq = ast.literal_eval(samples_df.loc[i, 'read_sequence'])
        consensus_seq = ast.literal_eval(samples_df.loc[i, 'consensus_sequence'])
        read_qual = ast.literal_eval(samples_df.loc[i, 'read_quality'])
        consensus_occur_freq = ast.literal_eval(samples_df.loc[i, 'consensus_occur_frequency'])
        consensus_avg_qual = ast.literal_eval(samples_df.loc[i, 'consensus_avg_quality'])
        true_labels = ast.literal_eval(samples_df.loc[i, 'true_labels'])
        
        seq_length = len(read_seq)
        base_position = list(range(seq_length))
    
        # normalize read_qual and consensus_avg_qual by max_quality, normalize base_position by seq_length-1
        read_qual = np.array(read_qual)/max_quality
        consensus_avg_qual = np.array(consensus_avg_qual)/max_quality
        base_position = np.array(base_position)/(seq_length-1)
        
        # get left and right kmers
        left_kmers_list, right_kmers_list = get_kmers_for_read_seq(read_seq, k, kmer_onehot_encoder)
        
        # features for the whole read: make them to go with timesteps
        read_length = [samples_df.loc[i, 'read_length']]*seq_length
        GC_content = [samples_df.loc[i, 'GC_content']]*seq_length
        num_spoa_reads = [samples_df.loc[i, 'number_spoa_reads']]*seq_length
        strand = [float(strand_code[samples_df.loc[i, 'strand']])]*seq_length
        
        # encode labels
        encoded_true_labels = [label_code[label] for label in true_labels]
        y += encoded_true_labels
        
        # divide longer reads into segments of max_seq_length
        if seq_length > max_seq_length:
            num_segments = math.ceil(seq_length/max_seq_length)
            for n in range(num_segments-1):   
                start = n*max_seq_length
                end = (n+1)*max_seq_length
                
                segm_read_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, read_seq[start:end])
                segm_consensus_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, consensus_seq[start:end])
                
                segm_read_qual = make_array_and_pad(read_qual[start:end], max_seq_length)
                segm_consensus_avg_qual = make_array_and_pad(consensus_avg_qual[start:end], max_seq_length)
                segm_base_position = make_array_and_pad(base_position[start:end], max_seq_length)
                segm_consensus_occur_freq = make_array_and_pad(np.array(consensus_occur_freq[start:end]), max_seq_length) 
                
                segm_read_length = make_array_and_pad(np.array(read_length[start:end]), max_seq_length)
                segm_GC_content = make_array_and_pad(np.array(GC_content[start:end]), max_seq_length)
                segm_num_spoa_reads = make_array_and_pad(np.array(num_spoa_reads[start:end]), max_seq_length)
                segm_strand = make_array_and_pad(np.array(strand[start:end]), max_seq_length)
                
                segm_left_kmers = pad_3d_array(np.array(left_kmers_list[start:end]), max_seq_length)
                segm_right_kmers = pad_3d_array(np.array(right_kmers_list[start:end]), max_seq_length)
                
                segm_encoded_true_labels = encode_labels(encoded_true_labels[start:end], max_seq_length)
                
                left_kmers_input.append(segm_left_kmers)
                right_kmers_input.append(segm_right_kmers)
                
                joint_read_features = np.concatenate((segm_read_seq, segm_read_qual, segm_base_position), axis=1)
                read_features_input.append(joint_read_features)
                
                joint_consensus_features = np.concatenate((segm_consensus_seq, segm_consensus_occur_freq, segm_consensus_avg_qual), axis=1)
                consensus_features_input.append(joint_consensus_features)
                
                joint_rest_features = np.concatenate((segm_read_length, segm_GC_content, segm_num_spoa_reads, segm_strand), axis=1)
                rest_features_input.append(joint_rest_features)
                
                true_labels_out.append(segm_encoded_true_labels)
                
                rows_this_read.append(row_id)
                row_id += 1
            
            # last segment
            final_start = (num_segments-1)*max_seq_length

            segm_read_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, read_seq[final_start:])
            segm_consensus_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, consensus_seq[final_start:])
            
            segm_read_qual = make_array_and_pad(read_qual[final_start:], max_seq_length)
            segm_consensus_avg_qual = make_array_and_pad(consensus_avg_qual[final_start:], max_seq_length)
            segm_base_position = make_array_and_pad(base_position[final_start:], max_seq_length)
            segm_consensus_occur_freq = make_array_and_pad(np.array(consensus_occur_freq[final_start:]), max_seq_length) 
            
            segm_read_length = make_array_and_pad(np.array(read_length[final_start:]), max_seq_length)
            segm_GC_content = make_array_and_pad(np.array(GC_content[final_start:]), max_seq_length)
            segm_num_spoa_reads = make_array_and_pad(np.array(num_spoa_reads[final_start:]), max_seq_length)
            segm_strand = make_array_and_pad(np.array(strand[final_start:]), max_seq_length)
            
            segm_left_kmers = pad_3d_array(np.array(left_kmers_list[final_start:]), max_seq_length)
            segm_right_kmers = pad_3d_array(np.array(right_kmers_list[final_start:]), max_seq_length)
            
            segm_encoded_true_labels = encode_labels(encoded_true_labels[final_start:], max_seq_length)
            
            left_kmers_input.append(segm_left_kmers)
            right_kmers_input.append(segm_right_kmers)
            
            joint_read_features = np.concatenate((segm_read_seq, segm_read_qual, segm_base_position), axis=1)
            read_features_input.append(joint_read_features)
            
            joint_consensus_features = np.concatenate((segm_consensus_seq, segm_consensus_occur_freq, segm_consensus_avg_qual), axis=1)
            consensus_features_input.append(joint_consensus_features)
            
            joint_rest_features = np.concatenate((segm_read_length, segm_GC_content, segm_num_spoa_reads, segm_strand), axis=1)
            rest_features_input.append(joint_rest_features)
            
            true_labels_out.append(segm_encoded_true_labels)   
            
            rows_this_read.append(row_id)
            row_id += 1
            
        else:
            # take the whole sequence
            segm_read_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, read_seq)
            segm_consensus_seq = encode_sequences(sequence_onehot_encoder, max_seq_length, consensus_seq)
            
            segm_read_qual = make_array_and_pad(read_qual, max_seq_length)
            segm_consensus_avg_qual = make_array_and_pad(consensus_avg_qual, max_seq_length)
            segm_base_position = make_array_and_pad(base_position, max_seq_length)
            segm_consensus_occur_freq = make_array_and_pad(np.array(consensus_occur_freq), max_seq_length) 
            
            segm_read_length = make_array_and_pad(np.array(read_length), max_seq_length)
            segm_GC_content = make_array_and_pad(np.array(GC_content), max_seq_length)
            segm_num_spoa_reads = make_array_and_pad(np.array(num_spoa_reads), max_seq_length)
            segm_strand = make_array_and_pad(np.array(strand), max_seq_length)
            
            segm_left_kmers = pad_3d_array(np.array(left_kmers_list), max_seq_length)
            segm_right_kmers = pad_3d_array(np.array(right_kmers_list), max_seq_length)
            
            segm_encoded_true_labels = encode_labels(encoded_true_labels, max_seq_length)
            
            left_kmers_input.append(segm_left_kmers)
            right_kmers_input.append(segm_right_kmers)
            
            joint_read_features = np.concatenate((segm_read_seq, segm_read_qual, segm_base_position), axis=1)
            read_features_input.append(joint_read_features)
            
            joint_consensus_features = np.concatenate((segm_consensus_seq, segm_consensus_occur_freq, segm_consensus_avg_qual), axis=1)
            consensus_features_input.append(joint_consensus_features)
            
            joint_rest_features = np.concatenate((segm_read_length, segm_GC_content, segm_num_spoa_reads, segm_strand), axis=1)
            rest_features_input.append(joint_rest_features)
            
            true_labels_out.append(segm_encoded_true_labels)     
            
            rows_this_read.append(row_id)
            row_id += 1
            
        read_rows_dict[read_name] = rows_this_read
        
    return np.array(left_kmers_input), np.array(right_kmers_input), np.array(read_features_input), np.array(consensus_features_input), np.array(rest_features_input), np.array(true_labels_out), read_rows_dict, np.array(y)
    

def make_array_and_pad(feat_array, max_seq_length):
    
    feat_array = feat_array.reshape(len(feat_array), 1)
    
    if max_seq_length == feat_array.shape[0]:
        seq_array = feat_array
    else:
        # padding -1
        seq_array = - np.ones((max_seq_length, feat_array.shape[-1]))
        seq_array[:feat_array.shape[0], :] = feat_array
        
    return seq_array    


def pad_3d_array(feat_array, max_seq_length):
    
    if max_seq_length == feat_array.shape[0]:
        seq_array = feat_array
    else:
        # padding -1
        seq_array = - np.ones((max_seq_length, feat_array.shape[1], feat_array.shape[2]))
        seq_array[:feat_array.shape[0], :] = feat_array
        
    return seq_array 
    

if __name__ == "__main__":

    clusters_sizes_file = sys.argv[1]
    fullpath_outdir = sys.argv[2]
    
    # constants
    max_spoa_size = 800
    max_quality = 93
    
    k = 5
    max_seq_length = 600
    
    # these are very changeable
    num_selected_clusters = 20000
    num_selected_reads_per_cluster = 80
    
    # get the clusters sizes info
    clusters_sizes_array = np.loadtxt(clusters_sizes_file, dtype='int')
    print("clusters_sizes_array shape:", clusters_sizes_array.shape)

    # randomly sample clusters
    row_idx_list = list(range(clusters_sizes_array.shape[0]))
    random.seed(24)
    selected_row_indices = random.sample(row_idx_list, num_selected_clusters)
    selected_row_indices.sort()
    selected_clusters_sizes_array = clusters_sizes_array[selected_row_indices]
    print("selected_clusters_sizes_array shape:", selected_clusters_sizes_array.shape)
    
    merged_df = get_data_from_selected_clusters(selected_clusters_sizes_array, max_spoa_size, num_selected_reads_per_cluster)
    print("merged_df:")
    print(merged_df.info())
    
    merged_df, max_read_length, min_read_length = normalize_data_columns(merged_df, max_spoa_size)
    print("max_read_length:", max_read_length)
    print("min_read_length:", min_read_length)
    
    train_df, test_df = train_test_split(merged_df, test_size=0.2, random_state=42)
    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)
    print("train_df:")
    print(train_df.info())
    print("test_df:")
    print(test_df.info())
    pd.DataFrame.to_csv(train_df, path_or_buf=fullpath_outdir + "/train_df.csv", index=False)
    pd.DataFrame.to_csv(test_df, path_or_buf=fullpath_outdir + "/test_df.csv", index=False)
    
    # one-hot encoders
    kmer_onehot_encoder = fit_onehot_encoder_kmers(kmer_bases)
    sequence_onehot_encoder = fit_onehot_encoder_sequences(sequence_bases)
    
    train_left_kmers_input, train_right_kmers_input, train_read_features_input, train_consensus_features_input, train_rest_features_input, train_true_labels, train_read_rows_dict, train_y = \
    construct_inputs_arrays(train_df, kmer_onehot_encoder, sequence_onehot_encoder, k, max_seq_length, max_quality)
    print("train_left_kmers_input shape:", train_left_kmers_input.shape)
    print("train_right_kmers_input shape:", train_right_kmers_input.shape)
    print("train_read_features_input shape:", train_read_features_input.shape)
    print("train_consensus_features_input shape:", train_consensus_features_input.shape)
    print("train_rest_features_input shape:", train_rest_features_input.shape)
    print("train_true_labels shape:", train_true_labels.shape)
    print("train_read_rows_dict size:", len(train_read_rows_dict))
    np.save(fullpath_outdir + "/train_left_kmers_input", train_left_kmers_input)
    np.save(fullpath_outdir + "/train_right_kmers_input", train_right_kmers_input)
    np.save(fullpath_outdir + "/train_read_features_input", train_read_features_input)
    np.save(fullpath_outdir + "/train_consensus_features_input", train_consensus_features_input)
    np.save(fullpath_outdir + "/train_rest_features_input", train_rest_features_input)
    np.save(fullpath_outdir + "/train_true_labels", train_true_labels)
    pickle_out = open(fullpath_outdir + "/train_read_rows_dict.pickle","wb")
    pickle.dump(train_read_rows_dict, pickle_out)
    pickle_out.close()
          
    test_left_kmers_input, test_right_kmers_input, test_read_features_input, test_consensus_features_input, test_rest_features_input, test_true_labels, test_read_rows_dict, test_y = \
    construct_inputs_arrays(test_df, kmer_onehot_encoder, sequence_onehot_encoder, k, max_seq_length, max_quality)
    print("test_left_kmers_input shape:", test_left_kmers_input.shape)
    print("test_right_kmers_input shape:", test_right_kmers_input.shape)
    print("test_read_features_input shape:", test_read_features_input.shape)
    print("test_consensus_features_input shape:", test_consensus_features_input.shape)
    print("test_rest_features_input shape:", test_rest_features_input.shape)
    print("test_true_labels shape:", test_true_labels.shape)
    print("test_read_rows_dict size:", len(test_read_rows_dict))
    np.save(fullpath_outdir + "/test_left_kmers_input", test_left_kmers_input)
    np.save(fullpath_outdir + "/test_right_kmers_input", test_right_kmers_input)
    np.save(fullpath_outdir + "/test_read_features_input", test_read_features_input)
    np.save(fullpath_outdir + "/test_consensus_features_input", test_consensus_features_input)
    np.save(fullpath_outdir + "/test_rest_features_input", test_rest_features_input)
    np.save(fullpath_outdir + "/test_true_labels", test_true_labels)
    pickle_out = open(fullpath_outdir + "/test_read_rows_dict.pickle","wb")
    pickle.dump(test_read_rows_dict, pickle_out)
    pickle_out.close()    
    
    # compute class weights and label distributions
    y_total = np.concatenate((train_y, test_y))
    class_labels = np.array(list(label_code.values()))
    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=y_total)
    class_weights_dict = dict(zip(class_labels, class_weights))
    print("class_weights_dict:", class_weights_dict)
    label_distributions = np.bincount(y_total)
    print("label_distributions:", label_distributions)
    pickle_out = open(fullpath_outdir + "/class_weights_dict.pickle","wb")
    pickle.dump(class_weights_dict, pickle_out)
    pickle_out.close()
    np.savetxt(fullpath_outdir + '/label_distributions.txt', label_distributions, fmt="%d")
    
    
